{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saikoukuntla/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/saikoukuntla/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from gensim import utils\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"public_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_humor</th>\n",
       "      <th>humor_rating</th>\n",
       "      <th>humor_controversy</th>\n",
       "      <th>offense_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TENNESSEE: We're the best state. Nobody even c...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A man inserted an advertisement in the classif...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>How many men does it take to open a can of bee...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Told my mom I hit 1200 Twitter followers. She ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Roses are dead. Love is fake. Weddings are bas...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>7996</td>\n",
       "      <td>Lack of awareness of the pervasiveness of raci...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>7997</td>\n",
       "      <td>Why are aspirins white? Because they work sorry</td>\n",
       "      <td>1</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>7998</td>\n",
       "      <td>Today, we Americans celebrate our independence...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>7999</td>\n",
       "      <td>How to keep the flies off the bride at an Ital...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>8000</td>\n",
       "      <td>\"Each ounce of sunflower seeds gives you 37% o...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  is_humor  \\\n",
       "0        1  TENNESSEE: We're the best state. Nobody even c...         1   \n",
       "1        2  A man inserted an advertisement in the classif...         1   \n",
       "2        3  How many men does it take to open a can of bee...         1   \n",
       "3        4  Told my mom I hit 1200 Twitter followers. She ...         1   \n",
       "4        5  Roses are dead. Love is fake. Weddings are bas...         1   \n",
       "...    ...                                                ...       ...   \n",
       "7995  7996  Lack of awareness of the pervasiveness of raci...         0   \n",
       "7996  7997    Why are aspirins white? Because they work sorry         1   \n",
       "7997  7998  Today, we Americans celebrate our independence...         1   \n",
       "7998  7999  How to keep the flies off the bride at an Ital...         1   \n",
       "7999  8000  \"Each ounce of sunflower seeds gives you 37% o...         0   \n",
       "\n",
       "      humor_rating  humor_controversy  offense_rating  \n",
       "0             2.42                1.0            0.20  \n",
       "1             2.50                1.0            1.10  \n",
       "2             1.95                0.0            2.40  \n",
       "3             2.11                1.0            0.00  \n",
       "4             2.78                0.0            0.10  \n",
       "...            ...                ...             ...  \n",
       "7995           NaN                NaN            0.25  \n",
       "7996          1.33                0.0            3.85  \n",
       "7997          2.55                0.0            0.00  \n",
       "7998          1.00                0.0            3.00  \n",
       "7999           NaN                NaN            0.00  \n",
       "\n",
       "[8000 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Processing\n",
    "#Lower-case all post\n",
    "# data.text = data.text.str.lower()\n",
    "\n",
    "#Remove handlers\n",
    "data.text = data.text.apply(lambda x:re.sub('@[^\\s]+','',x))\n",
    "test.text = test.text.apply(lambda x:re.sub('@[^\\s]+','',x))\n",
    "\n",
    "# Remove URLS\n",
    "data.text = data.text.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "test.text = data.text.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "\n",
    "# Remove all the special characters\n",
    "# data.text = df.text.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n",
    "\n",
    "#remove all single characters\n",
    "# df.text = df.text.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x))\n",
    "\n",
    "# Substituting multiple spaces with single space\n",
    "data.text = data.text.apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n",
    "test.text = data.text.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "sid = SIA()\n",
    "\n",
    "data['sentiments']           = data['text'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\n",
    "data['Compound Sentiment']            = data['sentiments'].apply(lambda x: x['compound']+1*(10**-6)) \n",
    "data['Positive Sentiment']   = data['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \n",
    "data['Neutral Sentiment']    = data['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\n",
    "data['Negative Sentiment']   = data['sentiments'].apply(lambda x: x['neg']+1*(10**-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "sid = SIA()\n",
    "\n",
    "test['sentiments']           = test['text'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\n",
    "test['Compound Sentiment']            = test['sentiments'].apply(lambda x: x['compound']+1*(10**-6)) \n",
    "test['Positive Sentiment']   = test['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \n",
    "test['Neutral Sentiment']    = test['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\n",
    "test['Negative Sentiment']   = test['sentiments'].apply(lambda x: x['neg']+1*(10**-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_humor</th>\n",
       "      <th>humor_rating</th>\n",
       "      <th>humor_controversy</th>\n",
       "      <th>offense_rating</th>\n",
       "      <th>Compound Sentiment</th>\n",
       "      <th>Positive Sentiment</th>\n",
       "      <th>Neutral Sentiment</th>\n",
       "      <th>Negative Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TENNESSEE: We're the best state. Nobody even c...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.153101</td>\n",
       "      <td>0.176001</td>\n",
       "      <td>0.672001</td>\n",
       "      <td>0.151001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A man inserted an advertisement in the classif...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.510601</td>\n",
       "      <td>0.099001</td>\n",
       "      <td>0.901001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>How many men does it take to open a can of bee...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Told my mom I hit 1200 Twitter followers. She ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Roses are dead. Love is fake. Weddings are bas...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.700299</td>\n",
       "      <td>0.189001</td>\n",
       "      <td>0.360001</td>\n",
       "      <td>0.450001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  is_humor  \\\n",
       "0   1  TENNESSEE: We're the best state. Nobody even c...         1   \n",
       "1   2  A man inserted an advertisement in the classif...         1   \n",
       "2   3  How many men does it take to open a can of bee...         1   \n",
       "3   4  Told my mom I hit 1200 Twitter followers. She ...         1   \n",
       "4   5  Roses are dead. Love is fake. Weddings are bas...         1   \n",
       "\n",
       "   humor_rating  humor_controversy  offense_rating  Compound Sentiment  \\\n",
       "0          2.42                1.0             0.2            0.153101   \n",
       "1          2.50                1.0             1.1            0.510601   \n",
       "2          1.95                0.0             2.4            0.000001   \n",
       "3          2.11                1.0             0.0            0.000001   \n",
       "4          2.78                0.0             0.1           -0.700299   \n",
       "\n",
       "   Positive Sentiment  Neutral Sentiment  Negative Sentiment  \n",
       "0            0.176001           0.672001            0.151001  \n",
       "1            0.099001           0.901001            0.000001  \n",
       "2            0.000001           1.000001            0.000001  \n",
       "3            0.000001           1.000001            0.000001  \n",
       "4            0.189001           0.360001            0.450001  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns='sentiments')\n",
    "test.drop(columns='sentiments')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.113144</td>\n",
       "      <td>0.118259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.128214</td>\n",
       "      <td>0.156543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.195250</td>\n",
       "      <td>0.195187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.652000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pos          neg\n",
       "count  1000.000000  1000.000000\n",
       "mean      0.113144     0.118259\n",
       "std       0.128214     0.156543\n",
       "min       0.000000     0.000000\n",
       "25%       0.000000     0.000000\n",
       "50%       0.084000     0.000000\n",
       "75%       0.195250     0.195187\n",
       "max       0.652000     1.000000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments = data[['Positive Sentiment','Negative Sentiment']]\n",
    "t_sentiments = test[['Positive Sentiment','Negative Sentiment']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "sentiments = scaler.fit_transform(sentiments)\n",
    "t_sentiments = scaler.transform(t_sentiments)\n",
    "sentiments = pd.DataFrame(data=sentiments,columns=['pos','neg'])\n",
    "t_sentiments = pd.DataFrame(data=t_sentiments,columns=['pos','neg'])\n",
    "t_sentiments.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram Accuracy: 0.8470\n",
      "1-gram F1-Score: 0.8807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count vectorizer\n",
    "cv = CountVectorizer(ngram_range=(1,2), binary=True)\n",
    "text_counts = cv.fit_transform(data['text'])\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "text_tfidf = tfidf.fit_transform(data['text'])\n",
    "\n",
    "count_vect_df = pd.DataFrame(text_counts.todense())\n",
    "tfidf_df = pd.DataFrame(text_tfidf.todense())\n",
    "\n",
    "\n",
    "df = pd.concat([count_vect_df, tfidf_df, sentiments], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, data['is_humor'], test_size=0.25, random_state=42)\n",
    "\n",
    "# Modeling\n",
    "CNB = MultinomialNB()\n",
    "CNB.fit(X_train, y_train)\n",
    "predicted = CNB.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(y_test, predicted)\n",
    "f1_score = metrics.f1_score(y_test, predicted)\n",
    "print(f'1-gram Accuracy: {accuracy_score:.4f}')\n",
    "print(f'1-gram F1-Score: {f1_score:.4f}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2), binary=True)\n",
    "text_counts = cv.fit_transform(data['text'])\n",
    "test_text_counts = cv.transform(test['text'])\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "text_tfidf = tfidf.fit_transform(data['text'])\n",
    "test_text_tfidf = tfidf.transform(test['text'])\n",
    "\n",
    "count_vect_df = pd.DataFrame(text_counts.todense())\n",
    "tfidf_df = pd.DataFrame(text_tfidf.todense())\n",
    "\n",
    "test_count_vect_df = pd.DataFrame(test_text_counts.todense())\n",
    "test_tfidf_df = pd.DataFrame(test_text_tfidf.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([count_vect_df, tfidf_df, sentiments], axis=1)\n",
    "test_df = pd.concat([test_count_vect_df, test_tfidf_df, t_sentiments],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4932, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc_data = data[data['is_humor']==1]\n",
    "sentiments = sentiments.loc[data['is_humor']==1,['positive','negative']]\n",
    "sentiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram Accuracy: 0.4834\n",
      "1-gram F1-Score: 0.4783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count vectorizer\n",
    "cv = CountVectorizer(ngram_range=(1,2), binary=True)\n",
    "text_counts = cv.fit_transform(hc_data['text'])\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "text_tfidf = tfidf.fit_transform(hc_data['text'])\n",
    "\n",
    "count_vect_df = pd.DataFrame(text_counts.todense())\n",
    "tfidf_df = pd.DataFrame(text_tfidf.todense())\n",
    "\n",
    "count_vect_df = count_vect_df.reset_index(drop=True)\n",
    "tfidf_df = tfidf_df.reset_index(drop=True)\n",
    "sentiments = sentiments.reset_index(drop=True)\n",
    "\n",
    "df = pd.concat([count_vect_df, tfidf_df, sentiments], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, hc_data['humor_controversy'], test_size=0.25, random_state=42)\n",
    "\n",
    "# Modeling\n",
    "CNB = MultinomialNB()\n",
    "CNB.fit(X_train, y_train)\n",
    "predicted = CNB.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(y_test, predicted)\n",
    "f1_score = metrics.f1_score(y_test, predicted)\n",
    "print(f'1-gram Accuracy: {accuracy_score:.4f}')\n",
    "print(f'1-gram F1-Score: {f1_score:.4f}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentiwordNet Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/saikoukuntla/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/saikoukuntla/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/saikoukuntla/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/saikoukuntla/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import sys\n",
    "import scipy\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tokenize import TreebankWordTokenizer, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB, MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "text = data['text']\n",
    "tokenized_text = [word_tokenize(i) for i in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tag and lemmatize words, then count pos/neg words in each entry\n",
    "# See https://nlpforhackers.io/sentiment-analysis-intro/ for details\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def swn_polarity(text):\n",
    "    \"\"\"\n",
    "    Return a pos and neg score\n",
    "    \"\"\"\n",
    " \n",
    "    pos_score = np.zeros(len(text))\n",
    "    neg_score = np.zeros(len(text))\n",
    "    pos_std = np.zeros(len(text))\n",
    "    neg_std = np.zeros(len(text))\n",
    "    \n",
    "    tokens_count = 0\n",
    "\n",
    "    for i in range(0,len(text)):\n",
    "        pos=[]\n",
    "        neg=[]\n",
    "        tagged_entry = pos_tag(text[i])\n",
    " \n",
    "        for word, tag in tagged_entry:\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "                continue\n",
    " \n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    " \n",
    "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "            if not synsets:\n",
    "                continue\n",
    " \n",
    "            # Take the first sense, the most common\n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    " \n",
    "            pos.append(swn_synset.pos_score()) \n",
    "            neg.append(swn_synset.neg_score())\n",
    "        \n",
    "        pos = np.array(pos)\n",
    "        neg = np.array(neg)\n",
    "        pos_score[i] = pos.sum()\n",
    "        neg_score[i] = neg.sum()\n",
    "        pos_std[i] = pos.std()\n",
    "        neg_std[i] = neg.std()\n",
    "\n",
    "    return pos_score, neg_score, pos_std, neg_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saikoukuntla/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:233: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/saikoukuntla/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:194: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(\n",
      "/Users/saikoukuntla/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.875 0.75  0.    ... 0.125 0.25  1.125] [0.25  0.    0.375 ... 0.    0.25  0.125] [0.23405972 0.13122266 0.         ... 0.04658475 0.0931695  0.22041775] [0.07856742 0.         0.13122266 ... 0.         0.0931695  0.04133986]\n"
     ]
    }
   ],
   "source": [
    "pos_score, neg_score, pos_std, neg_std = swn_polarity(tokenized_text)\n",
    "print(pos_score, neg_score, pos_std, neg_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pos_score'] = pos_score\n",
    "data['neg_score'] = neg_score\n",
    "data['pos_std'] = pos_std\n",
    "data['neg_std'] = neg_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pos_std'] = data['pos_std'].fillna(0)\n",
    "data['neg_std'] = data['neg_std'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_humor</th>\n",
       "      <th>humor_rating</th>\n",
       "      <th>humor_controversy</th>\n",
       "      <th>offense_rating</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>pos_std</th>\n",
       "      <th>neg_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TENNESSEE: We're the best state. Nobody even c...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.234060</td>\n",
       "      <td>0.078567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A man inserted an advertisement in the classif...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.131223</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>How many men does it take to open a can of bee...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Told my mom I hit 1200 Twitter followers. She ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.750</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.337483</td>\n",
       "      <td>0.143740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Roses are dead. Love is fake. Weddings are bas...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.245566</td>\n",
       "      <td>0.324760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  is_humor  \\\n",
       "0   1  TENNESSEE: We're the best state. Nobody even c...         1   \n",
       "1   2  A man inserted an advertisement in the classif...         1   \n",
       "2   3  How many men does it take to open a can of bee...         1   \n",
       "3   4  Told my mom I hit 1200 Twitter followers. She ...         1   \n",
       "4   5  Roses are dead. Love is fake. Weddings are bas...         1   \n",
       "\n",
       "   humor_rating  humor_controversy  offense_rating  pos_score  neg_score  \\\n",
       "0          2.42                1.0             0.2      0.875      0.250   \n",
       "1          2.50                1.0             1.1      0.750      0.000   \n",
       "2          1.95                0.0             2.4      0.000      0.375   \n",
       "3          2.11                1.0             0.0      1.750      0.500   \n",
       "4          2.78                0.0             0.1      1.125      1.500   \n",
       "\n",
       "    pos_std   neg_std  \n",
       "0  0.234060  0.078567  \n",
       "1  0.131223  0.000000  \n",
       "2  0.000000  0.131223  \n",
       "3  0.337483  0.143740  \n",
       "4  0.245566  0.324760  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['text', 'pos_score', 'neg_score', 'pos_std', 'neg_std']]\n",
    "y = data['is_humor']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "_treebank_word_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "def word_tokenize(text, language='english'):\n",
    "    \"\"\"\n",
    "    Return a tokenized copy of *text*,\n",
    "    using NLTK's recommended word tokenizer\n",
    "    (currently an improved :class:`.TreebankWordTokenizer`\n",
    "    along with :class:`.PunktSentenceTokenizer`\n",
    "    for the specified language).\n",
    "\n",
    "    :param text: text to split into words\n",
    "    :type text: str\n",
    "    :param language: the model name in the Punkt corpus\n",
    "    :type language: str\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text, language)\n",
    "    return [\n",
    "        token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), tokenizer=word_tokenize)\n",
    "count_vec_train = vectorizer.fit_transform(X_train['text'])\n",
    "count_vec_test = vectorizer.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_sentiment(count_vec, pos_score, neg_score, pos_std, neg_std):\n",
    "    columns = [str(i) for i in range(count_vec.shape[1])]\n",
    "    stack = pd.DataFrame(count_vec.toarray(), columns=columns)\n",
    "    stack['pos_score'] = pos_score.to_numpy()\n",
    "    stack['neg_score'] = neg_score.to_numpy()\n",
    "    stack['pos_std'] = pos_std.to_numpy()\n",
    "    stack['neg_std'] = neg_std.to_numpy()\n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stack = stack_sentiment(count_vec_train, X_train['pos_score'], X_train['neg_score'], X_train['pos_std'], X_train['neg_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_stack =  stack_sentiment(count_vec_test, X_test['pos_score'], X_test['neg_score'], X_test['pos_std'], X_test['neg_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.945377076083497\n",
      "Accuracy: 0.877\n",
      "F1 score: 0.9023034154090547\n"
     ]
    }
   ],
   "source": [
    "# Multinomial NB\n",
    "mnb = MultinomialNB(alpha=0.2).fit(X_train_stack, y_train)\n",
    "print(f\"AUC: {roc_auc_score(y_test, mnb.predict_proba(X_test_stack)[:, 1])}\")\n",
    "print(f\"Accuracy: {mnb.score(X_test_stack, y_test)}\")\n",
    "print(f\"F1 score: {f1_score(y_test, mnb.predict(X_test_stack))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9453490548381059\n",
      "Accuracy: 0.88\n",
      "F1 score: 0.904153354632588\n"
     ]
    }
   ],
   "source": [
    "# Complement NB\n",
    "cnb = ComplementNB(alpha=0.2).fit(X_train_stack, y_train)\n",
    "print(f\"AUC: {roc_auc_score(y_test, cnb.predict_proba(X_test_stack)[:, 1])}\")\n",
    "print(f\"Accuracy: {cnb.score(X_test_stack, y_test)}\")\n",
    "print(f\"F1 score: {f1_score(y_test, cnb.predict(X_test_stack))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
