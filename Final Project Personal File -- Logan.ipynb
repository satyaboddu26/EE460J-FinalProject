{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paper's authors' github:  https://github.com/Moradnejad/ColBERT-Using-BERT-Sentence-Embedding-for-Humor-Detection\n",
    "#to run this, you will need to have downloaded the Colbert model as indicated on the authors' github page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_25 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_27 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_29 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_31 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_32 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_33 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_34 (InputLayer)           [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_35 (InputLayer)           [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_36 (InputLayer)           [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (Custom>TFBertMainLayer)   multiple             109482240   input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "                                                                 input_22[0][0]                   \n",
      "                                                                 input_23[0][0]                   \n",
      "                                                                 input_24[0][0]                   \n",
      "                                                                 input_25[0][0]                   \n",
      "                                                                 input_26[0][0]                   \n",
      "                                                                 input_27[0][0]                   \n",
      "                                                                 input_28[0][0]                   \n",
      "                                                                 input_29[0][0]                   \n",
      "                                                                 input_30[0][0]                   \n",
      "                                                                 input_31[0][0]                   \n",
      "                                                                 input_32[0][0]                   \n",
      "                                                                 input_33[0][0]                   \n",
      "                                                                 input_34[0][0]                   \n",
      "                                                                 input_35[0][0]                   \n",
      "                                                                 input_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 768)          0           bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 768)          0           bert[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 768)          0           bert[2][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 768)          0           bert[3][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 768)          0           bert[4][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 768)          0           bert[5][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 32)           24608       global_average_pooling1d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 32)           24608       global_average_pooling1d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 32)           24608       global_average_pooling1d_8[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 32)           24608       global_average_pooling1d_9[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 32)           24608       global_average_pooling1d_10[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 256)          196864      global_average_pooling1d_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 32)           0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 32)           0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 32)           0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 32)           0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 32)           0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 256)          0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 8)            264         dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 8)            264         dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 8)            264         dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 8)            264         dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 8)            264         dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 64)           16448       dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 104)          0           dense_16[0][0]                   \n",
      "                                                                 dense_18[0][0]                   \n",
      "                                                                 dense_20[0][0]                   \n",
      "                                                                 dense_22[0][0]                   \n",
      "                                                                 dense_24[0][0]                   \n",
      "                                                                 dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 512)          53760       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 512)          0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 256)          131328      dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 1)            257         dense_28[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 110,005,257\n",
      "Trainable params: 110,005,257\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "model = keras.models.load_model(\"Colbert/colbert-trained/\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "# import bert_tokenization as tokenization\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras \n",
    "\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "from transformers import *\n",
    "\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re    #for regex\n",
    "\n",
    "import contractions\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9001</td>\n",
       "      <td>Finding out your ex got fat is like finding 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9002</td>\n",
       "      <td>For Brockmann, stereotypes imperil national se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9003</td>\n",
       "      <td>A girl runs up to her mother with a pile of cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9004</td>\n",
       "      <td>gotta wonder if baseball still would've been c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9005</td>\n",
       "      <td>When you're dreading getting in the shower cuz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text\n",
       "0  9001  Finding out your ex got fat is like finding 20...\n",
       "1  9002  For Brockmann, stereotypes imperil national se...\n",
       "2  9003  A girl runs up to her mother with a pile of cr...\n",
       "3  9004  gotta wonder if baseball still would've been c...\n",
       "4  9005  When you're dreading getting in the shower cuz..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"public_test.csv\")\n",
    "train_data.head()\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>TENNESSEE: We're the best state. Nobody even c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>A man inserted an advertisement in the classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>How many men does it take to open a can of bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Told my mom I hit 1200 Twitter followers. She ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Roses are dead. Love is fake. Weddings are bas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text\n",
       "0   1  TENNESSEE: We're the best state. Nobody even c...\n",
       "1   2  A man inserted an advertisement in the classif...\n",
       "2   3  How many men does it take to open a can of bee...\n",
       "3   4  Told my mom I hit 1200 Twitter followers. She ...\n",
       "4   5  Roses are dead. Love is fake. Weddings are bas..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_is_humor = train_data[\"is_humor\"]\n",
    "train_humor_rating = train_data[\"humor_rating\"]\n",
    "train_humor_controversy = train_data[\"humor_controversy\"]\n",
    "train_offense_rating = train_data[\"offense_rating\"]\n",
    "train_data = train_data.drop(columns = ['is_humor',\"humor_rating\", \"humor_controversy\", \"offense_rating\"])\n",
    "\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man inserted an advertisement in the classifieds \"Wife Wanted\". The next day, he received 1000 of replies, all reading: \"You can have mine.\" Free delivery also available at your door step\n"
     ]
    }
   ],
   "source": [
    "print(train_data.iloc[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.iloc[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "word_list = train_data.iloc[1,1].split()\n",
    "number_of_words = len(word_list)\n",
    "print(number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing data to match what the paper did\n",
    "\n",
    "indices = []\n",
    "for i in range(len(train_data)):\n",
    "    word_list = train_data.iloc[i,1].split()\n",
    "    number_of_words = len(word_list)\n",
    "    if (((len(train_data.iloc[i,1]) > 29) & (len(train_data.iloc[i,1]) < 101)) & ((number_of_words > 9) & (number_of_words < 19))):\n",
    "        indices.append(i)\n",
    "    train_data.iloc[i,1] = contractions.fix(train_data.iloc[i,1])\n",
    "    train_data.iloc[i,1] = train_data.iloc[i, 1].capitalize().capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2893"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Told my mom i hit 1200 twitter followers. she pointed out how my brother owns a house and i am wanted by several collection agencies. oh ma!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.iloc[3,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.iloc[3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.iloc[indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.iloc[3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_is_humor = train_is_humor.iloc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_is_humor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing test data as specified by paper\n",
    "\n",
    "indices = []\n",
    "for i in range(len(test_data)):\n",
    "    word_list = test_data.iloc[i,1].split()\n",
    "    number_of_words = len(word_list)\n",
    "    if (((len(test_data.iloc[i,1]) > 29) & (len(test_data.iloc[i,1]) < 101)) & ((number_of_words > 9) & (number_of_words < 19))):\n",
    "        indices.append(i)\n",
    "    test_data.iloc[i,1] = contractions.fix(test_data.iloc[i,1])\n",
    "    test_data.iloc[i,1] = test_data.iloc[i, 1].capitalize().capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.iloc[indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_is_humor, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: is_humor, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_is_humor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1467    1\n",
       "5768    0\n",
       "5714    1\n",
       "1578    0\n",
       "6958    1\n",
       "Name: is_humor, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "MODEL_TYPE = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lwing\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 20\n",
    "MAX_SENTENCES = 5\n",
    "MAX_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "    inputs = tokenizer.encode_plus(str1, str2,\n",
    "        add_special_tokens=True,\n",
    "        max_length=length,\n",
    "        truncation_strategy=truncation_strategy)\n",
    "\n",
    "    input_ids =  inputs[\"input_ids\"]\n",
    "    input_masks = [1] * len(input_ids)\n",
    "    input_segments = inputs[\"token_type_ids\"]\n",
    "    padding_length = length - len(input_ids)\n",
    "    padding_id = tokenizer.pad_token_id\n",
    "    input_ids = input_ids + ([padding_id] * padding_length)\n",
    "    input_masks = input_masks + ([0] * padding_length)\n",
    "    input_segments = input_segments + ([0] * padding_length)\n",
    "\n",
    "    return [input_ids, input_masks, input_segments]\n",
    "\n",
    "\n",
    "def compute_input_arrays(df, columns, tokenizer):\n",
    "    model_input = []\n",
    "    for xx in range((MAX_SENTENCES*3)+3):\n",
    "        model_input.append([])\n",
    "    \n",
    "    for _, row in tqdm(df[columns].iterrows()):\n",
    "        i = 0\n",
    "        \n",
    "        # sent\n",
    "        sentences = sent_tokenize(row.text)\n",
    "        for xx in range(MAX_SENTENCES):\n",
    "            s = sentences[xx] if xx<len(sentences) else ''\n",
    "            ids_q, masks_q, segments_q = return_id(s, None, 'longest_first', MAX_SENTENCE_LENGTH)\n",
    "            model_input[i].append(ids_q)\n",
    "            i+=1\n",
    "            model_input[i].append(masks_q)\n",
    "            i+=1\n",
    "            model_input[i].append(segments_q)\n",
    "            i+=1\n",
    "        \n",
    "        # full row\n",
    "        ids_q, masks_q, segments_q = return_id(row.text, None, 'longest_first', MAX_LENGTH)\n",
    "        model_input[i].append(ids_q)\n",
    "        i+=1\n",
    "        model_input[i].append(masks_q)\n",
    "        i+=1\n",
    "        model_input[i].append(segments_q)\n",
    "        \n",
    "    for xx in range((MAX_SENTENCES*3)+3):\n",
    "        model_input[xx] = np.asarray(model_input[xx], dtype=np.int32)\n",
    "        \n",
    "    print(model_input[0].shape)\n",
    "    return model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b580e7cd1484887bdae09e92bcbf584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(2314, 20)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0605fe881f3645b5b7ad0a0fc488df82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(579, 20)\n"
     ]
    }
   ],
   "source": [
    "inputs      = compute_input_arrays(X_train, ['text'], tokenizer)\n",
    "test_inputs = compute_input_arrays(X_test, ['text'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: what do you call a swimming pool full of blondes? a: frosted flakes.\n",
      "['Q: what do you call a swimming pool full of blondes?', 'a: frosted flakes.']\n"
     ]
    }
   ],
   "source": [
    "xx = 7\n",
    "print(X_train.iloc[xx,1])\n",
    "print(sent_tokenize(X_train.iloc[xx,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train.to_frame()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])\n",
    "\n",
    "outputs = compute_output_arrays(y_train.to_frame(), ['is_humor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "import sklearn\n",
    "def print_evaluation_metrics(y_true, y_pred, label='', is_regression=True, label2=''):\n",
    "    print('==================', label2)\n",
    "    ### For regression\n",
    "    if is_regression:\n",
    "        print('mean_absolute_error',label,':', sklearn.metrics.mean_absolute_error(y_true, y_pred))\n",
    "        print('mean_squared_error',label,':', sklearn.metrics.mean_squared_error(y_true, y_pred))\n",
    "        print('r2 score',label,':', sklearn.metrics.r2_score(y_true, y_pred))\n",
    "        #     print('max_error',label,':', sklearn.metrics.max_error(y_true, y_pred))\n",
    "        return sklearn.metrics.mean_squared_error(y_true, y_pred)\n",
    "    else:\n",
    "        ### FOR Classification\n",
    "#         print('balanced_accuracy_score',label,':', sklearn.metrics.balanced_accuracy_score(y_true, y_pred))\n",
    "#         print('average_precision_score',label,':', sklearn.metrics.average_precision_score(y_true, y_pred))\n",
    "#         print('balanced_accuracy_score',label,':', sklearn.metrics.balanced_accuracy_score(y_true, y_pred))\n",
    "#         print('accuracy_score',label,':', sklearn.metrics.accuracy_score(y_true, y_pred))\n",
    "        print('f1_score',label,':', sklearn.metrics.f1_score(y_true, y_pred))\n",
    "        \n",
    "        matrix = sklearn.metrics.confusion_matrix(y_true, y_pred)\n",
    "        print(matrix)\n",
    "        TP,TN,FP,FN = matrix[1][1],matrix[0][0],matrix[0][1],matrix[1][0]\n",
    "        Accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
    "        Precision = TP/(TP+FP)\n",
    "        Recall = TP/(TP+FN)\n",
    "        F1 = 2*(Recall * Precision) / (Recall + Precision)\n",
    "        print('Acc', Accuracy, 'Prec', Precision, 'Rec', Recall, 'F1',F1)\n",
    "        return sklearn.metrics.accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_inputs = inputs\n",
    "valid_outputs = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[25].trainable = True\n",
    "# model.layers[26].trainable = True\n",
    "# model.layers[27].trainable = True\n",
    "# model.layers[28].trainable = True\n",
    "# model.layers[29].trainable = True\n",
    "# model.layers[30].trainable = True\n",
    "# model.layers[31].trainable = True\n",
    "# model.layers[32].trainable = True\n",
    "# model.layers[33].trainable = True\n",
    "# model.layers[34].trainable = True\n",
    "# model.layers[35].trainable = True\n",
    "# model.layers[36].trainable = True\n",
    "# model.layers[37].trainable = True\n",
    "# model.layers[38].trainable = True\n",
    "# model.layers[39].trainable = True\n",
    "# model.layers[40].trainable = True\n",
    "# model.layers[41].trainable = True\n",
    "# model.layers[42].trainable = True\n",
    "# model.layers[43].trainable = True\n",
    "# model.layers[44].trainable = True\n",
    "model.layers[45].trainable = True\n",
    "model.layers[46].trainable = True\n",
    "model.layers[47].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "66/66 [==============================] - ETA: 28:27 - loss: 1.5582 - binary_accuracy: 0.78 - ETA: 6:13 - loss: 1.6563 - binary_accuracy: 0.7656 - ETA: 6:19 - loss: 1.7310 - binary_accuracy: 0.753 - ETA: 6:28 - loss: 1.7434 - binary_accuracy: 0.750 - ETA: 6:36 - loss: 1.7476 - binary_accuracy: 0.749 - ETA: 6:27 - loss: 1.7331 - binary_accuracy: 0.751 - ETA: 6:19 - loss: 1.7276 - binary_accuracy: 0.751 - ETA: 6:13 - loss: 1.7256 - binary_accuracy: 0.752 - ETA: 6:07 - loss: 1.7128 - binary_accuracy: 0.754 - ETA: 6:02 - loss: 1.7135 - binary_accuracy: 0.754 - ETA: 5:55 - loss: 1.7153 - binary_accuracy: 0.753 - ETA: 5:47 - loss: 1.7182 - binary_accuracy: 0.753 - ETA: 5:40 - loss: 1.7204 - binary_accuracy: 0.753 - ETA: 5:33 - loss: 1.7221 - binary_accuracy: 0.752 - ETA: 5:27 - loss: 1.7266 - binary_accuracy: 0.752 - ETA: 5:19 - loss: 1.7306 - binary_accuracy: 0.751 - ETA: 5:13 - loss: 1.7378 - binary_accuracy: 0.750 - ETA: 5:06 - loss: 1.7439 - binary_accuracy: 0.750 - ETA: 4:59 - loss: 1.7468 - binary_accuracy: 0.749 - ETA: 4:53 - loss: 1.7502 - binary_accuracy: 0.749 - ETA: 4:46 - loss: 1.7526 - binary_accuracy: 0.748 - ETA: 4:40 - loss: 1.7575 - binary_accuracy: 0.748 - ETA: 4:34 - loss: 1.7602 - binary_accuracy: 0.747 - ETA: 4:27 - loss: 1.7627 - binary_accuracy: 0.747 - ETA: 4:21 - loss: 1.7640 - binary_accuracy: 0.747 - ETA: 4:14 - loss: 1.7653 - binary_accuracy: 0.747 - ETA: 4:08 - loss: 1.7664 - binary_accuracy: 0.746 - ETA: 4:01 - loss: 1.7681 - binary_accuracy: 0.746 - ETA: 3:55 - loss: 1.7695 - binary_accuracy: 0.746 - ETA: 3:49 - loss: 1.7707 - binary_accuracy: 0.746 - ETA: 3:42 - loss: 1.7719 - binary_accuracy: 0.746 - ETA: 3:36 - loss: 1.7741 - binary_accuracy: 0.745 - ETA: 3:29 - loss: 1.7767 - binary_accuracy: 0.745 - ETA: 3:23 - loss: 1.7791 - binary_accuracy: 0.744 - ETA: 3:17 - loss: 1.7816 - binary_accuracy: 0.744 - ETA: 3:10 - loss: 1.7845 - binary_accuracy: 0.744 - ETA: 3:04 - loss: 1.7876 - binary_accuracy: 0.743 - ETA: 2:58 - loss: 1.7912 - binary_accuracy: 0.743 - ETA: 2:51 - loss: 1.7950 - binary_accuracy: 0.742 - ETA: 2:45 - loss: 1.7986 - binary_accuracy: 0.742 - ETA: 2:39 - loss: 1.8012 - binary_accuracy: 0.741 - ETA: 2:33 - loss: 1.8036 - binary_accuracy: 0.741 - ETA: 2:27 - loss: 1.8059 - binary_accuracy: 0.740 - ETA: 2:20 - loss: 1.8082 - binary_accuracy: 0.740 - ETA: 2:14 - loss: 1.8105 - binary_accuracy: 0.740 - ETA: 2:08 - loss: 1.8133 - binary_accuracy: 0.739 - ETA: 2:01 - loss: 1.8161 - binary_accuracy: 0.739 - ETA: 1:55 - loss: 1.8192 - binary_accuracy: 0.738 - ETA: 1:49 - loss: 1.8221 - binary_accuracy: 0.738 - ETA: 1:42 - loss: 1.8244 - binary_accuracy: 0.738 - ETA: 1:36 - loss: 1.8267 - binary_accuracy: 0.737 - ETA: 1:30 - loss: 1.8285 - binary_accuracy: 0.737 - ETA: 1:23 - loss: 1.8300 - binary_accuracy: 0.737 - ETA: 1:17 - loss: 1.8312 - binary_accuracy: 0.736 - ETA: 1:10 - loss: 1.8323 - binary_accuracy: 0.736 - ETA: 1:04 - loss: 1.8329 - binary_accuracy: 0.736 - ETA: 58s - loss: 1.8338 - binary_accuracy: 0.736 - ETA: 51s - loss: 1.8347 - binary_accuracy: 0.73 - ETA: 45s - loss: 1.8356 - binary_accuracy: 0.73 - ETA: 38s - loss: 1.8365 - binary_accuracy: 0.73 - ETA: 32s - loss: 1.8374 - binary_accuracy: 0.73 - ETA: 25s - loss: 1.8384 - binary_accuracy: 0.73 - ETA: 19s - loss: 1.8392 - binary_accuracy: 0.73 - ETA: 12s - loss: 1.8400 - binary_accuracy: 0.73 - ETA: 6s - loss: 1.8408 - binary_accuracy: 0.7346 - ETA: 0s - loss: 1.8416 - binary_accuracy: 0.734 - 490s 7s/step - loss: 1.8423 - binary_accuracy: 0.7343 - val_loss: 2.1879 - val_binary_accuracy: 0.7026\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - ETA: 7:03 - loss: 1.2919 - binary_accuracy: 0.781 - ETA: 6:58 - loss: 1.4846 - binary_accuracy: 0.765 - ETA: 6:49 - loss: 1.6515 - binary_accuracy: 0.750 - ETA: 6:45 - loss: 1.6888 - binary_accuracy: 0.748 - ETA: 6:38 - loss: 1.7122 - binary_accuracy: 0.745 - ETA: 6:34 - loss: 1.7057 - binary_accuracy: 0.746 - ETA: 6:30 - loss: 1.7150 - binary_accuracy: 0.745 - ETA: 6:23 - loss: 1.7340 - binary_accuracy: 0.742 - ETA: 6:18 - loss: 1.7408 - binary_accuracy: 0.742 - ETA: 6:10 - loss: 1.7418 - binary_accuracy: 0.742 - ETA: 6:06 - loss: 1.7395 - binary_accuracy: 0.742 - ETA: 5:59 - loss: 1.7370 - binary_accuracy: 0.742 - ETA: 5:52 - loss: 1.7354 - binary_accuracy: 0.742 - ETA: 5:46 - loss: 1.7350 - binary_accuracy: 0.742 - ETA: 5:38 - loss: 1.7365 - binary_accuracy: 0.741 - ETA: 5:32 - loss: 1.7412 - binary_accuracy: 0.741 - ETA: 5:25 - loss: 1.7454 - binary_accuracy: 0.740 - ETA: 5:18 - loss: 1.7483 - binary_accuracy: 0.740 - ETA: 5:11 - loss: 1.7497 - binary_accuracy: 0.739 - ETA: 5:05 - loss: 1.7539 - binary_accuracy: 0.739 - ETA: 4:58 - loss: 1.7551 - binary_accuracy: 0.738 - ETA: 4:52 - loss: 1.7568 - binary_accuracy: 0.738 - ETA: 4:46 - loss: 1.7576 - binary_accuracy: 0.738 - ETA: 4:39 - loss: 1.7596 - binary_accuracy: 0.737 - ETA: 4:33 - loss: 1.7604 - binary_accuracy: 0.737 - ETA: 4:26 - loss: 1.7596 - binary_accuracy: 0.737 - ETA: 4:18 - loss: 1.7600 - binary_accuracy: 0.737 - ETA: 4:11 - loss: 1.7606 - binary_accuracy: 0.737 - ETA: 4:04 - loss: 1.7603 - binary_accuracy: 0.737 - ETA: 3:57 - loss: 1.7596 - binary_accuracy: 0.737 - ETA: 3:50 - loss: 1.7593 - binary_accuracy: 0.737 - ETA: 3:43 - loss: 1.7602 - binary_accuracy: 0.737 - ETA: 3:36 - loss: 1.7611 - binary_accuracy: 0.737 - ETA: 3:29 - loss: 1.7621 - binary_accuracy: 0.736 - ETA: 3:22 - loss: 1.7627 - binary_accuracy: 0.736 - ETA: 3:16 - loss: 1.7633 - binary_accuracy: 0.736 - ETA: 3:09 - loss: 1.7636 - binary_accuracy: 0.736 - ETA: 3:02 - loss: 1.7636 - binary_accuracy: 0.736 - ETA: 2:55 - loss: 1.7638 - binary_accuracy: 0.736 - ETA: 2:49 - loss: 1.7643 - binary_accuracy: 0.736 - ETA: 2:42 - loss: 1.7653 - binary_accuracy: 0.736 - ETA: 2:35 - loss: 1.7659 - binary_accuracy: 0.736 - ETA: 2:29 - loss: 1.7668 - binary_accuracy: 0.735 - ETA: 2:22 - loss: 1.7679 - binary_accuracy: 0.735 - ETA: 2:16 - loss: 1.7685 - binary_accuracy: 0.735 - ETA: 2:09 - loss: 1.7691 - binary_accuracy: 0.735 - ETA: 2:02 - loss: 1.7698 - binary_accuracy: 0.735 - ETA: 1:56 - loss: 1.7705 - binary_accuracy: 0.735 - ETA: 1:49 - loss: 1.7714 - binary_accuracy: 0.735 - ETA: 1:43 - loss: 1.7724 - binary_accuracy: 0.735 - ETA: 1:36 - loss: 1.7733 - binary_accuracy: 0.735 - ETA: 1:30 - loss: 1.7744 - binary_accuracy: 0.735 - ETA: 1:23 - loss: 1.7753 - binary_accuracy: 0.734 - ETA: 1:17 - loss: 1.7763 - binary_accuracy: 0.734 - ETA: 1:11 - loss: 1.7772 - binary_accuracy: 0.734 - ETA: 1:04 - loss: 1.7780 - binary_accuracy: 0.734 - ETA: 58s - loss: 1.7790 - binary_accuracy: 0.734 - ETA: 52s - loss: 1.7801 - binary_accuracy: 0.73 - ETA: 45s - loss: 1.7811 - binary_accuracy: 0.73 - ETA: 39s - loss: 1.7823 - binary_accuracy: 0.73 - ETA: 32s - loss: 1.7837 - binary_accuracy: 0.73 - ETA: 26s - loss: 1.7849 - binary_accuracy: 0.73 - ETA: 19s - loss: 1.7864 - binary_accuracy: 0.73 - ETA: 13s - loss: 1.7876 - binary_accuracy: 0.73 - ETA: 6s - loss: 1.7891 - binary_accuracy: 0.7336 - ETA: 0s - loss: 1.7906 - binary_accuracy: 0.733 - 478s 7s/step - loss: 1.7920 - binary_accuracy: 0.7332 - val_loss: 2.1879 - val_binary_accuracy: 0.7026\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - ETA: 8:34 - loss: 1.6594 - binary_accuracy: 0.781 - ETA: 8:25 - loss: 1.6913 - binary_accuracy: 0.781 - ETA: 8:25 - loss: 1.6901 - binary_accuracy: 0.781 - ETA: 8:18 - loss: 1.7243 - binary_accuracy: 0.777 - ETA: 7:53 - loss: 1.7596 - binary_accuracy: 0.773 - ETA: 7:42 - loss: 1.7491 - binary_accuracy: 0.773 - ETA: 7:29 - loss: 1.7505 - binary_accuracy: 0.772 - ETA: 7:16 - loss: 1.7540 - binary_accuracy: 0.770 - ETA: 7:02 - loss: 1.7572 - binary_accuracy: 0.768 - ETA: 6:54 - loss: 1.7561 - binary_accuracy: 0.767 - ETA: 6:44 - loss: 1.7569 - binary_accuracy: 0.765 - ETA: 6:35 - loss: 1.7556 - binary_accuracy: 0.764 - ETA: 6:30 - loss: 1.7521 - binary_accuracy: 0.762 - ETA: 6:20 - loss: 1.7481 - binary_accuracy: 0.762 - ETA: 6:12 - loss: 1.7497 - binary_accuracy: 0.760 - ETA: 6:04 - loss: 1.7511 - binary_accuracy: 0.759 - ETA: 5:56 - loss: 1.7520 - binary_accuracy: 0.759 - ETA: 5:47 - loss: 1.7500 - binary_accuracy: 0.758 - ETA: 5:39 - loss: 1.7484 - binary_accuracy: 0.757 - ETA: 5:32 - loss: 1.7494 - binary_accuracy: 0.757 - ETA: 5:27 - loss: 1.7485 - binary_accuracy: 0.756 - ETA: 5:21 - loss: 1.7468 - binary_accuracy: 0.756 - ETA: 5:13 - loss: 1.7460 - binary_accuracy: 0.755 - ETA: 5:07 - loss: 1.7460 - binary_accuracy: 0.755 - ETA: 5:01 - loss: 1.7441 - binary_accuracy: 0.755 - ETA: 4:53 - loss: 1.7439 - binary_accuracy: 0.754 - ETA: 4:48 - loss: 1.7441 - binary_accuracy: 0.754 - ETA: 4:40 - loss: 1.7458 - binary_accuracy: 0.754 - ETA: 4:32 - loss: 1.7477 - binary_accuracy: 0.753 - ETA: 4:25 - loss: 1.7501 - binary_accuracy: 0.752 - ETA: 4:17 - loss: 1.7528 - binary_accuracy: 0.752 - ETA: 4:09 - loss: 1.7545 - binary_accuracy: 0.751 - ETA: 4:01 - loss: 1.7556 - binary_accuracy: 0.751 - ETA: 3:53 - loss: 1.7555 - binary_accuracy: 0.751 - ETA: 3:45 - loss: 1.7550 - binary_accuracy: 0.750 - ETA: 3:37 - loss: 1.7541 - binary_accuracy: 0.750 - ETA: 3:30 - loss: 1.7537 - binary_accuracy: 0.750 - ETA: 3:22 - loss: 1.7532 - binary_accuracy: 0.750 - ETA: 3:14 - loss: 1.7525 - binary_accuracy: 0.749 - ETA: 3:07 - loss: 1.7522 - binary_accuracy: 0.749 - ETA: 2:59 - loss: 1.7526 - binary_accuracy: 0.749 - ETA: 2:52 - loss: 1.7536 - binary_accuracy: 0.749 - ETA: 2:45 - loss: 1.7547 - binary_accuracy: 0.748 - ETA: 2:38 - loss: 1.7561 - binary_accuracy: 0.748 - ETA: 2:31 - loss: 1.7577 - binary_accuracy: 0.748 - ETA: 2:24 - loss: 1.7595 - binary_accuracy: 0.747 - ETA: 2:16 - loss: 1.7618 - binary_accuracy: 0.747 - ETA: 2:09 - loss: 1.7644 - binary_accuracy: 0.746 - ETA: 2:03 - loss: 1.7668 - binary_accuracy: 0.746 - ETA: 1:55 - loss: 1.7690 - binary_accuracy: 0.746 - ETA: 1:48 - loss: 1.7709 - binary_accuracy: 0.745 - ETA: 1:41 - loss: 1.7729 - binary_accuracy: 0.745 - ETA: 1:33 - loss: 1.7750 - binary_accuracy: 0.744 - ETA: 1:26 - loss: 1.7768 - binary_accuracy: 0.744 - ETA: 1:19 - loss: 1.7786 - binary_accuracy: 0.744 - ETA: 1:12 - loss: 1.7803 - binary_accuracy: 0.744 - ETA: 1:04 - loss: 1.7820 - binary_accuracy: 0.743 - ETA: 57s - loss: 1.7838 - binary_accuracy: 0.743 - ETA: 50s - loss: 1.7853 - binary_accuracy: 0.74 - ETA: 43s - loss: 1.7868 - binary_accuracy: 0.74 - ETA: 35s - loss: 1.7884 - binary_accuracy: 0.74 - ETA: 28s - loss: 1.7899 - binary_accuracy: 0.74 - ETA: 21s - loss: 1.7913 - binary_accuracy: 0.74 - ETA: 14s - loss: 1.7928 - binary_accuracy: 0.74 - ETA: 7s - loss: 1.7943 - binary_accuracy: 0.7414 - ETA: 0s - loss: 1.7958 - binary_accuracy: 0.741 - 510s 8s/step - loss: 1.7972 - binary_accuracy: 0.7409 - val_loss: 2.1879 - val_binary_accuracy: 0.7026\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - ETA: 7:10 - loss: 1.4291 - binary_accuracy: 0.750 - ETA: 7:23 - loss: 1.6454 - binary_accuracy: 0.726 - ETA: 7:06 - loss: 1.7883 - binary_accuracy: 0.720 - ETA: 6:59 - loss: 1.8561 - binary_accuracy: 0.716 - ETA: 6:53 - loss: 1.9138 - binary_accuracy: 0.709 - ETA: 6:45 - loss: 1.9582 - binary_accuracy: 0.704 - ETA: 6:39 - loss: 2.0029 - binary_accuracy: 0.701 - ETA: 6:31 - loss: 2.0290 - binary_accuracy: 0.699 - ETA: 6:26 - loss: 2.0417 - binary_accuracy: 0.698 - ETA: 6:23 - loss: 2.0394 - binary_accuracy: 0.699 - ETA: 6:15 - loss: 2.0328 - binary_accuracy: 0.701 - ETA: 6:08 - loss: 2.0268 - binary_accuracy: 0.703 - ETA: 6:00 - loss: 2.0185 - binary_accuracy: 0.704 - ETA: 5:54 - loss: 2.0100 - binary_accuracy: 0.705 - ETA: 5:46 - loss: 2.0010 - binary_accuracy: 0.707 - ETA: 5:40 - loss: 1.9951 - binary_accuracy: 0.708 - ETA: 5:33 - loss: 1.9892 - binary_accuracy: 0.709 - ETA: 5:25 - loss: 1.9819 - binary_accuracy: 0.710 - ETA: 5:19 - loss: 1.9760 - binary_accuracy: 0.711 - ETA: 5:11 - loss: 1.9720 - binary_accuracy: 0.712 - ETA: 5:06 - loss: 1.9676 - binary_accuracy: 0.712 - ETA: 4:59 - loss: 1.9620 - binary_accuracy: 0.713 - ETA: 4:52 - loss: 1.9587 - binary_accuracy: 0.714 - ETA: 4:45 - loss: 1.9576 - binary_accuracy: 0.714 - ETA: 4:37 - loss: 1.9565 - binary_accuracy: 0.715 - ETA: 4:31 - loss: 1.9571 - binary_accuracy: 0.715 - ETA: 4:26 - loss: 1.9583 - binary_accuracy: 0.715 - ETA: 4:20 - loss: 1.9584 - binary_accuracy: 0.715 - ETA: 4:12 - loss: 1.9592 - binary_accuracy: 0.716 - ETA: 4:05 - loss: 1.9600 - binary_accuracy: 0.716 - ETA: 3:59 - loss: 1.9619 - binary_accuracy: 0.716 - ETA: 3:52 - loss: 1.9638 - binary_accuracy: 0.716 - ETA: 3:45 - loss: 1.9647 - binary_accuracy: 0.716 - ETA: 3:38 - loss: 1.9665 - binary_accuracy: 0.716 - ETA: 3:32 - loss: 1.9684 - binary_accuracy: 0.716 - ETA: 3:25 - loss: 1.9692 - binary_accuracy: 0.716 - ETA: 3:19 - loss: 1.9689 - binary_accuracy: 0.716 - ETA: 3:12 - loss: 1.9684 - binary_accuracy: 0.716 - ETA: 3:05 - loss: 1.9680 - binary_accuracy: 0.716 - ETA: 2:58 - loss: 1.9686 - binary_accuracy: 0.716 - ETA: 2:51 - loss: 1.9691 - binary_accuracy: 0.716 - ETA: 2:44 - loss: 1.9700 - binary_accuracy: 0.716 - ETA: 2:38 - loss: 1.9710 - binary_accuracy: 0.716 - ETA: 2:31 - loss: 1.9716 - binary_accuracy: 0.716 - ETA: 2:24 - loss: 1.9719 - binary_accuracy: 0.716 - ETA: 2:17 - loss: 1.9716 - binary_accuracy: 0.716 - ETA: 2:10 - loss: 1.9716 - binary_accuracy: 0.716 - ETA: 2:03 - loss: 1.9713 - binary_accuracy: 0.717 - ETA: 1:56 - loss: 1.9709 - binary_accuracy: 0.717 - ETA: 1:49 - loss: 1.9701 - binary_accuracy: 0.717 - ETA: 1:42 - loss: 1.9691 - binary_accuracy: 0.717 - ETA: 1:35 - loss: 1.9683 - binary_accuracy: 0.717 - ETA: 1:28 - loss: 1.9671 - binary_accuracy: 0.717 - ETA: 1:22 - loss: 1.9657 - binary_accuracy: 0.718 - ETA: 1:15 - loss: 1.9644 - binary_accuracy: 0.718 - ETA: 1:08 - loss: 1.9632 - binary_accuracy: 0.718 - ETA: 1:01 - loss: 1.9620 - binary_accuracy: 0.718 - ETA: 55s - loss: 1.9607 - binary_accuracy: 0.719 - ETA: 48s - loss: 1.9592 - binary_accuracy: 0.71 - ETA: 41s - loss: 1.9577 - binary_accuracy: 0.71 - ETA: 34s - loss: 1.9565 - binary_accuracy: 0.71 - ETA: 27s - loss: 1.9556 - binary_accuracy: 0.71 - ETA: 20s - loss: 1.9546 - binary_accuracy: 0.71 - ETA: 13s - loss: 1.9536 - binary_accuracy: 0.71 - ETA: 6s - loss: 1.9526 - binary_accuracy: 0.7199 - ETA: 0s - loss: 1.9517 - binary_accuracy: 0.720 - 489s 7s/step - loss: 1.9508 - binary_accuracy: 0.7201 - val_loss: 2.1879 - val_binary_accuracy: 0.7026\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - ETA: 7:02 - loss: 1.4257 - binary_accuracy: 0.843 - ETA: 6:35 - loss: 1.6176 - binary_accuracy: 0.796 - ETA: 6:33 - loss: 1.6943 - binary_accuracy: 0.774 - ETA: 6:28 - loss: 1.7540 - binary_accuracy: 0.760 - ETA: 6:20 - loss: 1.8109 - binary_accuracy: 0.749 - ETA: 6:15 - loss: 1.8928 - binary_accuracy: 0.736 - ETA: 6:08 - loss: 1.9619 - binary_accuracy: 0.727 - ETA: 6:02 - loss: 2.0060 - binary_accuracy: 0.721 - ETA: 5:57 - loss: 2.0289 - binary_accuracy: 0.717 - ETA: 5:50 - loss: 2.0401 - binary_accuracy: 0.715 - ETA: 5:45 - loss: 2.0426 - binary_accuracy: 0.714 - ETA: 5:38 - loss: 2.0380 - binary_accuracy: 0.714 - ETA: 5:32 - loss: 2.0373 - binary_accuracy: 0.713 - ETA: 5:26 - loss: 2.0350 - binary_accuracy: 0.713 - ETA: 5:19 - loss: 2.0377 - binary_accuracy: 0.712 - ETA: 5:15 - loss: 2.0421 - binary_accuracy: 0.711 - ETA: 5:09 - loss: 2.0454 - binary_accuracy: 0.710 - ETA: 5:04 - loss: 2.0460 - binary_accuracy: 0.710 - ETA: 4:58 - loss: 2.0460 - binary_accuracy: 0.710 - ETA: 4:51 - loss: 2.0450 - binary_accuracy: 0.710 - ETA: 4:46 - loss: 2.0434 - binary_accuracy: 0.710 - ETA: 4:41 - loss: 2.0420 - binary_accuracy: 0.710 - ETA: 4:34 - loss: 2.0398 - binary_accuracy: 0.710 - ETA: 4:28 - loss: 2.0377 - binary_accuracy: 0.710 - ETA: 4:22 - loss: 2.0347 - binary_accuracy: 0.710 - ETA: 4:15 - loss: 2.0321 - binary_accuracy: 0.710 - ETA: 4:09 - loss: 2.0283 - binary_accuracy: 0.710 - ETA: 4:02 - loss: 2.0243 - binary_accuracy: 0.711 - ETA: 3:56 - loss: 2.0194 - binary_accuracy: 0.711 - ETA: 3:49 - loss: 2.0154 - binary_accuracy: 0.712 - ETA: 3:43 - loss: 2.0103 - binary_accuracy: 0.712 - ETA: 3:37 - loss: 2.0051 - binary_accuracy: 0.713 - ETA: 3:30 - loss: 2.0004 - binary_accuracy: 0.713 - ETA: 3:24 - loss: 1.9962 - binary_accuracy: 0.714 - ETA: 3:17 - loss: 1.9917 - binary_accuracy: 0.714 - ETA: 3:11 - loss: 1.9881 - binary_accuracy: 0.714 - ETA: 3:05 - loss: 1.9858 - binary_accuracy: 0.715 - ETA: 2:58 - loss: 1.9831 - binary_accuracy: 0.715 - ETA: 2:52 - loss: 1.9798 - binary_accuracy: 0.715 - ETA: 2:46 - loss: 1.9764 - binary_accuracy: 0.716 - ETA: 2:41 - loss: 1.9733 - binary_accuracy: 0.716 - ETA: 2:34 - loss: 1.9700 - binary_accuracy: 0.717 - ETA: 2:28 - loss: 1.9675 - binary_accuracy: 0.717 - ETA: 2:21 - loss: 1.9652 - binary_accuracy: 0.717 - ETA: 2:15 - loss: 1.9631 - binary_accuracy: 0.718 - ETA: 2:08 - loss: 1.9610 - binary_accuracy: 0.718 - ETA: 2:02 - loss: 1.9593 - binary_accuracy: 0.718 - ETA: 1:56 - loss: 1.9576 - binary_accuracy: 0.718 - ETA: 1:49 - loss: 1.9560 - binary_accuracy: 0.718 - ETA: 1:43 - loss: 1.9545 - binary_accuracy: 0.718 - ETA: 1:37 - loss: 1.9527 - binary_accuracy: 0.718 - ETA: 1:30 - loss: 1.9510 - binary_accuracy: 0.719 - ETA: 1:24 - loss: 1.9496 - binary_accuracy: 0.719 - ETA: 1:17 - loss: 1.9485 - binary_accuracy: 0.719 - ETA: 1:11 - loss: 1.9473 - binary_accuracy: 0.719 - ETA: 1:04 - loss: 1.9460 - binary_accuracy: 0.719 - ETA: 58s - loss: 1.9450 - binary_accuracy: 0.719 - ETA: 51s - loss: 1.9441 - binary_accuracy: 0.71 - ETA: 45s - loss: 1.9436 - binary_accuracy: 0.71 - ETA: 38s - loss: 1.9433 - binary_accuracy: 0.71 - ETA: 32s - loss: 1.9428 - binary_accuracy: 0.71 - ETA: 25s - loss: 1.9424 - binary_accuracy: 0.71 - ETA: 19s - loss: 1.9420 - binary_accuracy: 0.71 - ETA: 12s - loss: 1.9415 - binary_accuracy: 0.71 - ETA: 6s - loss: 1.9410 - binary_accuracy: 0.7198 - ETA: 0s - loss: 1.9405 - binary_accuracy: 0.719 - 468s 7s/step - loss: 1.9400 - binary_accuracy: 0.7199 - val_loss: 2.1879 - val_binary_accuracy: 0.7026\n",
      "Epoch 6/10\n",
      "66/66 [==============================] - ETA: 7:12 - loss: 2.0727 - binary_accuracy: 0.718 - ETA: 7:01 - loss: 2.0061 - binary_accuracy: 0.718 - ETA: 7:14 - loss: 1.9496 - binary_accuracy: 0.722 - ETA: 7:05 - loss: 1.9356 - binary_accuracy: 0.723 - ETA: 7:23 - loss: 1.9063 - binary_accuracy: 0.727 - ETA: 7:11 - loss: 1.8693 - binary_accuracy: 0.732 - ETA: 7:06 - loss: 1.8554 - binary_accuracy: 0.736 - ETA: 7:03 - loss: 1.8511 - binary_accuracy: 0.736 - ETA: 7:03 - loss: 1.8475 - binary_accuracy: 0.737 - ETA: 6:52 - loss: 1.8479 - binary_accuracy: 0.737 - ETA: 6:49 - loss: 1.8506 - binary_accuracy: 0.737 - ETA: 6:42 - loss: 1.8475 - binary_accuracy: 0.737 - ETA: 6:31 - loss: 1.8394 - binary_accuracy: 0.739 - ETA: 6:22 - loss: 1.8292 - binary_accuracy: 0.740 - ETA: 6:12 - loss: 1.8221 - binary_accuracy: 0.741 - ETA: 6:03 - loss: 1.8172 - binary_accuracy: 0.742 - ETA: 5:54 - loss: 1.8132 - binary_accuracy: 0.742 - ETA: 5:46 - loss: 1.8077 - binary_accuracy: 0.743 - ETA: 5:37 - loss: 1.8042 - binary_accuracy: 0.743 - ETA: 5:29 - loss: 1.7998 - binary_accuracy: 0.744 - ETA: 5:21 - loss: 1.7950 - binary_accuracy: 0.745 - ETA: 5:13 - loss: 1.7910 - binary_accuracy: 0.745 - ETA: 5:06 - loss: 1.7874 - binary_accuracy: 0.746 - ETA: 4:58 - loss: 1.7855 - binary_accuracy: 0.746 - ETA: 4:51 - loss: 1.7834 - binary_accuracy: 0.747 - ETA: 4:45 - loss: 1.7831 - binary_accuracy: 0.747 - ETA: 4:40 - loss: 1.7821 - binary_accuracy: 0.747 - ETA: 4:32 - loss: 1.7817 - binary_accuracy: 0.747 - ETA: 4:25 - loss: 1.7810 - binary_accuracy: 0.747 - ETA: 4:17 - loss: 1.7800 - binary_accuracy: 0.747 - ETA: 4:10 - loss: 1.7786 - binary_accuracy: 0.747 - ETA: 4:03 - loss: 1.7778 - binary_accuracy: 0.747 - ETA: 3:56 - loss: 1.7769 - binary_accuracy: 0.747 - ETA: 3:49 - loss: 1.7769 - binary_accuracy: 0.747 - ETA: 3:42 - loss: 1.7774 - binary_accuracy: 0.747 - ETA: 3:34 - loss: 1.7777 - binary_accuracy: 0.747 - ETA: 3:27 - loss: 1.7778 - binary_accuracy: 0.747 - ETA: 3:19 - loss: 1.7778 - binary_accuracy: 0.746 - ETA: 3:12 - loss: 1.7788 - binary_accuracy: 0.746 - ETA: 3:05 - loss: 1.7801 - binary_accuracy: 0.746 - ETA: 2:57 - loss: 1.7826 - binary_accuracy: 0.745 - ETA: 2:50 - loss: 1.7846 - binary_accuracy: 0.745 - ETA: 2:43 - loss: 1.7869 - binary_accuracy: 0.745 - ETA: 2:35 - loss: 1.7892 - binary_accuracy: 0.744 - ETA: 2:28 - loss: 1.7919 - binary_accuracy: 0.744 - ETA: 2:21 - loss: 1.7945 - binary_accuracy: 0.743 - ETA: 2:13 - loss: 1.7970 - binary_accuracy: 0.743 - ETA: 2:06 - loss: 1.7989 - binary_accuracy: 0.743 - ETA: 1:59 - loss: 1.8011 - binary_accuracy: 0.742 - ETA: 1:52 - loss: 1.8030 - binary_accuracy: 0.742 - ETA: 1:45 - loss: 1.8047 - binary_accuracy: 0.742 - ETA: 1:38 - loss: 1.8063 - binary_accuracy: 0.741 - ETA: 1:30 - loss: 1.8076 - binary_accuracy: 0.741 - ETA: 1:23 - loss: 1.8084 - binary_accuracy: 0.741 - ETA: 1:16 - loss: 1.8090 - binary_accuracy: 0.741 - ETA: 1:09 - loss: 1.8100 - binary_accuracy: 0.740 - ETA: 1:02 - loss: 1.8114 - binary_accuracy: 0.740 - ETA: 55s - loss: 1.8127 - binary_accuracy: 0.740 - ETA: 48s - loss: 1.8140 - binary_accuracy: 0.74 - ETA: 41s - loss: 1.8153 - binary_accuracy: 0.73 - ETA: 34s - loss: 1.8166 - binary_accuracy: 0.73 - ETA: 28s - loss: 1.8180 - binary_accuracy: 0.73 - ETA: 21s - loss: 1.8191 - binary_accuracy: 0.73 - ETA: 14s - loss: 1.8201 - binary_accuracy: 0.73 - ETA: 7s - loss: 1.8211 - binary_accuracy: 0.7387 - ETA: 0s - loss: 1.8220 - binary_accuracy: 0.738 - 506s 8s/step - loss: 1.8230 - binary_accuracy: 0.7383 - val_loss: 2.1879 - val_binary_accuracy: 0.7026\n",
      "Epoch 7/10\n",
      "66/66 [==============================] - ETA: 8:02 - loss: 2.3155 - binary_accuracy: 0.718 - ETA: 8:01 - loss: 2.1587 - binary_accuracy: 0.726 - ETA: 7:48 - loss: 2.0685 - binary_accuracy: 0.727 - ETA: 7:36 - loss: 2.0525 - binary_accuracy: 0.725 - ETA: 7:21 - loss: 2.0418 - binary_accuracy: 0.721 - ETA: 7:15 - loss: 2.0422 - binary_accuracy: 0.717 - ETA: 7:04 - loss: 2.0374 - binary_accuracy: 0.716 - ETA: 7:01 - loss: 2.0130 - binary_accuracy: 0.718 - ETA: 6:52 - loss: 2.0045 - binary_accuracy: 0.718 - ETA: 6:42 - loss: 2.0101 - binary_accuracy: 0.717 - ETA: 6:29 - loss: 2.0096 - binary_accuracy: 0.717 - ETA: 6:18 - loss: 2.0022 - binary_accuracy: 0.717 - ETA: 6:08 - loss: 1.9975 - binary_accuracy: 0.717 - ETA: 5:59 - loss: 1.9906 - binary_accuracy: 0.718 - ETA: 5:50 - loss: 1.9844 - binary_accuracy: 0.719 - ETA: 5:41 - loss: 1.9804 - binary_accuracy: 0.720 - ETA: 5:33 - loss: 1.9748 - binary_accuracy: 0.721 - ETA: 5:25 - loss: 1.9691 - binary_accuracy: 0.722 - ETA: 5:17 - loss: 1.9628 - binary_accuracy: 0.723 - ETA: 5:09 - loss: 1.9597 - binary_accuracy: 0.723 - ETA: 5:01 - loss: 1.9551 - binary_accuracy: 0.724 - ETA: 4:54 - loss: 1.9526 - binary_accuracy: 0.725 - ETA: 4:47 - loss: 1.9493 - binary_accuracy: 0.725 - ETA: 4:39 - loss: 1.9474 - binary_accuracy: 0.726 - ETA: 4:32 - loss: 1.9439 - binary_accuracy: 0.726 - ETA: 4:25 - loss: 1.9405 - binary_accuracy: 0.727 - ETA: 4:17 - loss: 1.9372 - binary_accuracy: 0.727 - ETA: 4:10 - loss: 1.9353 - binary_accuracy: 0.728 - ETA: 4:03 - loss: 1.9334 - binary_accuracy: 0.728 - ETA: 3:56 - loss: 1.9316 - binary_accuracy: 0.728 - ETA: 3:50 - loss: 1.9309 - binary_accuracy: 0.728 - ETA: 3:43 - loss: 1.9309 - binary_accuracy: 0.728 - ETA: 3:36 - loss: 1.9308 - binary_accuracy: 0.728 - ETA: 3:29 - loss: 1.9305 - binary_accuracy: 0.728 - ETA: 3:22 - loss: 1.9300 - binary_accuracy: 0.728 - ETA: 3:16 - loss: 1.9293 - binary_accuracy: 0.728 - ETA: 3:09 - loss: 1.9282 - binary_accuracy: 0.729 - ETA: 3:02 - loss: 1.9279 - binary_accuracy: 0.729 - ETA: 2:56 - loss: 1.9278 - binary_accuracy: 0.729 - ETA: 2:49 - loss: 1.9280 - binary_accuracy: 0.729 - ETA: 2:42 - loss: 1.9280 - binary_accuracy: 0.729 - ETA: 2:36 - loss: 1.9286 - binary_accuracy: 0.728 - ETA: 2:29 - loss: 1.9293 - binary_accuracy: 0.728 - ETA: 2:22 - loss: 1.9302 - binary_accuracy: 0.728 - ETA: 2:16 - loss: 1.9306 - binary_accuracy: 0.728 - ETA: 2:09 - loss: 1.9308 - binary_accuracy: 0.728 - ETA: 2:03 - loss: 1.9309 - binary_accuracy: 0.728 - ETA: 1:56 - loss: 1.9315 - binary_accuracy: 0.728 - ETA: 1:50 - loss: 1.9318 - binary_accuracy: 0.728 - ETA: 1:43 - loss: 1.9319 - binary_accuracy: 0.728 - ETA: 1:37 - loss: 1.9316 - binary_accuracy: 0.728 - ETA: 1:30 - loss: 1.9311 - binary_accuracy: 0.728 - ETA: 1:24 - loss: 1.9308 - binary_accuracy: 0.728 - ETA: 1:17 - loss: 1.9305 - binary_accuracy: 0.728 - ETA: 1:11 - loss: 1.9301 - binary_accuracy: 0.728 - ETA: 1:04 - loss: 1.9297 - binary_accuracy: 0.728 - ETA: 58s - loss: 1.9294 - binary_accuracy: 0.728 - ETA: 51s - loss: 1.9290 - binary_accuracy: 0.72 - ETA: 45s - loss: 1.9284 - binary_accuracy: 0.72 - ETA: 38s - loss: 1.9279 - binary_accuracy: 0.72 - ETA: 32s - loss: 1.9274 - binary_accuracy: 0.72 - ETA: 25s - loss: 1.9267 - binary_accuracy: 0.72 - ETA: 19s - loss: 1.9261 - binary_accuracy: 0.72 - ETA: 12s - loss: 1.9258 - binary_accuracy: 0.72 - ETA: 6s - loss: 1.9258 - binary_accuracy: 0.7282 - ETA: 0s - loss: 1.9257 - binary_accuracy: 0.728 - 461s 7s/step - loss: 1.9256 - binary_accuracy: 0.7281 - val_loss: 2.1879 - val_binary_accuracy: 0.7026\n",
      "Epoch 8/10\n",
      "66/66 [==============================] - ETA: 6:44 - loss: 2.9888 - binary_accuracy: 0.531 - ETA: 6:59 - loss: 2.8541 - binary_accuracy: 0.562 - ETA: 6:42 - loss: 2.7059 - binary_accuracy: 0.586 - ETA: 6:37 - loss: 2.5616 - binary_accuracy: 0.612 - ETA: 6:29 - loss: 2.4795 - binary_accuracy: 0.629 - ETA: 6:20 - loss: 2.4119 - binary_accuracy: 0.643 - ETA: 6:16 - loss: 2.3772 - binary_accuracy: 0.652 - ETA: 6:08 - loss: 2.3386 - binary_accuracy: 0.661 - ETA: 6:02 - loss: 2.3026 - binary_accuracy: 0.668 - ETA: 5:55 - loss: 2.2784 - binary_accuracy: 0.674 - ETA: 5:48 - loss: 2.2501 - binary_accuracy: 0.679 - ETA: 5:42 - loss: 2.2276 - binary_accuracy: 0.684 - ETA: 5:35 - loss: 2.2009 - binary_accuracy: 0.689 - ETA: 5:29 - loss: 2.1791 - binary_accuracy: 0.693 - ETA: 5:23 - loss: 2.1582 - binary_accuracy: 0.696 - ETA: 5:16 - loss: 2.1400 - binary_accuracy: 0.699 - ETA: 5:10 - loss: 2.1223 - binary_accuracy: 0.702 - ETA: 5:03 - loss: 2.1039 - binary_accuracy: 0.704 - ETA: 4:56 - loss: 2.0862 - binary_accuracy: 0.707 - ETA: 4:50 - loss: 2.0705 - binary_accuracy: 0.709 - ETA: 4:43 - loss: 2.0568 - binary_accuracy: 0.711 - ETA: 4:37 - loss: 2.0446 - binary_accuracy: 0.712 - ETA: 4:31 - loss: 2.0320 - binary_accuracy: 0.714 - ETA: 4:24 - loss: 2.0205 - binary_accuracy: 0.715 - ETA: 4:18 - loss: 2.0109 - binary_accuracy: 0.716 - ETA: 4:12 - loss: 2.0026 - binary_accuracy: 0.717 - ETA: 4:06 - loss: 1.9955 - binary_accuracy: 0.718 - ETA: 4:00 - loss: 1.9888 - binary_accuracy: 0.719 - ETA: 3:53 - loss: 1.9830 - binary_accuracy: 0.720 - ETA: 3:47 - loss: 1.9769 - binary_accuracy: 0.720 - ETA: 3:41 - loss: 1.9720 - binary_accuracy: 0.721 - ETA: 3:35 - loss: 1.9684 - binary_accuracy: 0.721 - ETA: 3:28 - loss: 1.9655 - binary_accuracy: 0.722 - ETA: 3:22 - loss: 1.9627 - binary_accuracy: 0.722 - ETA: 3:15 - loss: 1.9602 - binary_accuracy: 0.722 - ETA: 3:09 - loss: 1.9579 - binary_accuracy: 0.722 - ETA: 3:03 - loss: 1.9556 - binary_accuracy: 0.722 - ETA: 2:56 - loss: 1.9534 - binary_accuracy: 0.722 - ETA: 2:50 - loss: 1.9514 - binary_accuracy: 0.723 - ETA: 2:44 - loss: 1.9492 - binary_accuracy: 0.723 - ETA: 2:37 - loss: 1.9467 - binary_accuracy: 0.723 - ETA: 2:31 - loss: 1.9444 - binary_accuracy: 0.723 - ETA: 2:25 - loss: 1.9424 - binary_accuracy: 0.723 - ETA: 2:18 - loss: 1.9405 - binary_accuracy: 0.723 - ETA: 2:12 - loss: 1.9385 - binary_accuracy: 0.724 - ETA: 2:06 - loss: 1.9367 - binary_accuracy: 0.724 - ETA: 1:59 - loss: 1.9349 - binary_accuracy: 0.724 - ETA: 1:53 - loss: 1.9332 - binary_accuracy: 0.724 - ETA: 1:47 - loss: 1.9318 - binary_accuracy: 0.724 - ETA: 1:40 - loss: 1.9303 - binary_accuracy: 0.724 - ETA: 1:34 - loss: 1.9292 - binary_accuracy: 0.724 - ETA: 1:28 - loss: 1.9281 - binary_accuracy: 0.724 - ETA: 1:22 - loss: 1.9274 - binary_accuracy: 0.724 - ETA: 1:15 - loss: 1.9269 - binary_accuracy: 0.724 - ETA: 1:09 - loss: 1.9265 - binary_accuracy: 0.724 - ETA: 1:03 - loss: 1.9261 - binary_accuracy: 0.724 - ETA: 56s - loss: 1.9259 - binary_accuracy: 0.724 - ETA: 50s - loss: 1.9259 - binary_accuracy: 0.72 - ETA: 44s - loss: 1.9259 - binary_accuracy: 0.72 - ETA: 37s - loss: 1.9258 - binary_accuracy: 0.72 - ETA: 31s - loss: 1.9257 - binary_accuracy: 0.72 - ETA: 25s - loss: 1.9253 - binary_accuracy: 0.72 - ETA: 18s - loss: 1.9249 - binary_accuracy: 0.72 - ETA: 12s - loss: 1.9246 - binary_accuracy: 0.72 - ETA: 6s - loss: 1.9243 - binary_accuracy: 0.7242 - ETA: 0s - loss: 1.9240 - binary_accuracy: 0.724 - 449s 7s/step - loss: 1.9237 - binary_accuracy: 0.7242 - val_loss: 2.1879 - val_binary_accuracy: 0.7026\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - ETA: 6:45 - loss: 2.2825 - binary_accuracy: 0.718 - ETA: 6:54 - loss: 2.1761 - binary_accuracy: 0.726 - ETA: 6:39 - loss: 2.1666 - binary_accuracy: 0.720 - ETA: 6:36 - loss: 2.1236 - binary_accuracy: 0.722 - ETA: 6:26 - loss: 2.1281 - binary_accuracy: 0.717 - ETA: 6:17 - loss: 2.1312 - binary_accuracy: 0.715 - ETA: 6:13 - loss: 2.1377 - binary_accuracy: 0.713 - ETA: 6:05 - loss: 2.1359 - binary_accuracy: 0.711 - ETA: 5:59 - loss: 2.1338 - binary_accuracy: 0.710 - ETA: 5:52 - loss: 2.1320 - binary_accuracy: 0.710 - ETA: 5:45 - loss: 2.1290 - binary_accuracy: 0.709 - ETA: 5:40 - loss: 2.1219 - binary_accuracy: 0.709 - ETA: 5:34 - loss: 2.1187 - binary_accuracy: 0.709 - ETA: 5:27 - loss: 2.1106 - binary_accuracy: 0.710 - ETA: 5:21 - loss: 2.1024 - binary_accuracy: 0.711 - ETA: 5:14 - loss: 2.0952 - binary_accuracy: 0.711 - ETA: 5:09 - loss: 2.0847 - binary_accuracy: 0.712 - ETA: 5:02 - loss: 2.0764 - binary_accuracy: 0.713 - ETA: 4:56 - loss: 2.0677 - binary_accuracy: 0.713 - ETA: 4:50 - loss: 2.0596 - binary_accuracy: 0.714 - ETA: 4:43 - loss: 2.0521 - binary_accuracy: 0.714 - ETA: 4:37 - loss: 2.0458 - binary_accuracy: 0.715 - ETA: 4:30 - loss: 2.0389 - binary_accuracy: 0.715 - ETA: 4:24 - loss: 2.0332 - binary_accuracy: 0.716 - ETA: 4:18 - loss: 2.0287 - binary_accuracy: 0.716 - ETA: 4:11 - loss: 2.0228 - binary_accuracy: 0.717 - ETA: 4:05 - loss: 2.0167 - binary_accuracy: 0.717 - ETA: 3:59 - loss: 2.0098 - binary_accuracy: 0.718 - ETA: 3:52 - loss: 2.0032 - binary_accuracy: 0.718 - ETA: 3:47 - loss: 1.9973 - binary_accuracy: 0.719 - ETA: 3:42 - loss: 1.9912 - binary_accuracy: 0.720 - ETA: 3:36 - loss: 1.9860 - binary_accuracy: 0.720 - ETA: 3:30 - loss: 1.9813 - binary_accuracy: 0.721 - ETA: 3:24 - loss: 1.9771 - binary_accuracy: 0.721 - ETA: 3:18 - loss: 1.9728 - binary_accuracy: 0.722 - ETA: 3:12 - loss: 1.9696 - binary_accuracy: 0.722 - ETA: 3:06 - loss: 1.9666 - binary_accuracy: 0.722 - ETA: 3:00 - loss: 1.9630 - binary_accuracy: 0.723 - ETA: 2:54 - loss: 1.9599 - binary_accuracy: 0.723 - ETA: 2:47 - loss: 1.9575 - binary_accuracy: 0.723 - ETA: 2:41 - loss: 1.9554 - binary_accuracy: 0.723 - ETA: 2:35 - loss: 1.9535 - binary_accuracy: 0.723 - ETA: 2:28 - loss: 1.9525 - binary_accuracy: 0.723 - ETA: 2:22 - loss: 1.9512 - binary_accuracy: 0.723 - ETA: 2:16 - loss: 1.9496 - binary_accuracy: 0.724 - ETA: 2:10 - loss: 1.9483 - binary_accuracy: 0.724 - ETA: 2:03 - loss: 1.9479 - binary_accuracy: 0.724 - ETA: 1:57 - loss: 1.9475 - binary_accuracy: 0.724 - ETA: 1:50 - loss: 1.9470 - binary_accuracy: 0.724 - ETA: 1:44 - loss: 1.9467 - binary_accuracy: 0.723 - ETA: 1:38 - loss: 1.9464 - binary_accuracy: 0.723 - ETA: 1:31 - loss: 1.9462 - binary_accuracy: 0.723 - ETA: 1:25 - loss: 1.9457 - binary_accuracy: 0.723 - ETA: 1:18 - loss: 1.9453 - binary_accuracy: 0.723 - ETA: 1:12 - loss: 1.9446 - binary_accuracy: 0.723 - ETA: 1:05 - loss: 1.9439 - binary_accuracy: 0.723 - ETA: 59s - loss: 1.9430 - binary_accuracy: 0.723 - ETA: 52s - loss: 1.9421 - binary_accuracy: 0.72 - ETA: 46s - loss: 1.9415 - binary_accuracy: 0.72 - ETA: 39s - loss: 1.9409 - binary_accuracy: 0.72 - ETA: 33s - loss: 1.9405 - binary_accuracy: 0.72 - ETA: 26s - loss: 1.9400 - binary_accuracy: 0.72 - ETA: 19s - loss: 1.9395 - binary_accuracy: 0.72 - ETA: 13s - loss: 1.9387 - binary_accuracy: 0.72 - ETA: 6s - loss: 1.9382 - binary_accuracy: 0.7238 - ETA: 0s - loss: 1.9377 - binary_accuracy: 0.723 - 473s 7s/step - loss: 1.9372 - binary_accuracy: 0.7238 - val_loss: 2.1879 - val_binary_accuracy: 0.7026\n",
      "Epoch 10/10\n",
      "66/66 [==============================] - ETA: 7:14 - loss: 0.8805 - binary_accuracy: 0.843 - ETA: 7:10 - loss: 1.2998 - binary_accuracy: 0.789 - ETA: 7:08 - loss: 1.5097 - binary_accuracy: 0.765 - ETA: 6:59 - loss: 1.5882 - binary_accuracy: 0.757 - ETA: 6:56 - loss: 1.6364 - binary_accuracy: 0.752 - ETA: 6:46 - loss: 1.6818 - binary_accuracy: 0.746 - ETA: 6:40 - loss: 1.7126 - binary_accuracy: 0.742 - ETA: 6:33 - loss: 1.7284 - binary_accuracy: 0.741 - ETA: 6:25 - loss: 1.7391 - binary_accuracy: 0.740 - ETA: 6:19 - loss: 1.7529 - binary_accuracy: 0.739 - ETA: 6:12 - loss: 1.7619 - binary_accuracy: 0.739 - ETA: 6:06 - loss: 1.7746 - binary_accuracy: 0.738 - ETA: 5:58 - loss: 1.7817 - binary_accuracy: 0.737 - ETA: 5:52 - loss: 1.7908 - binary_accuracy: 0.736 - ETA: 5:45 - loss: 1.7996 - binary_accuracy: 0.735 - ETA: 5:38 - loss: 1.8062 - binary_accuracy: 0.735 - ETA: 5:32 - loss: 1.8151 - binary_accuracy: 0.734 - ETA: 5:25 - loss: 1.8236 - binary_accuracy: 0.733 - ETA: 5:18 - loss: 1.8325 - binary_accuracy: 0.732 - ETA: 5:12 - loss: 1.8405 - binary_accuracy: 0.731 - ETA: 5:06 - loss: 1.8492 - binary_accuracy: 0.730 - ETA: 4:59 - loss: 1.8561 - binary_accuracy: 0.729 - ETA: 4:52 - loss: 1.8609 - binary_accuracy: 0.729 - ETA: 4:46 - loss: 1.8635 - binary_accuracy: 0.729 - ETA: 4:38 - loss: 1.8668 - binary_accuracy: 0.728 - ETA: 4:32 - loss: 1.8713 - binary_accuracy: 0.728 - ETA: 4:25 - loss: 1.8748 - binary_accuracy: 0.728 - ETA: 4:18 - loss: 1.8774 - binary_accuracy: 0.728 - ETA: 4:11 - loss: 1.8789 - binary_accuracy: 0.728 - ETA: 4:04 - loss: 1.8800 - binary_accuracy: 0.728 - ETA: 3:58 - loss: 1.8818 - binary_accuracy: 0.728 - ETA: 3:51 - loss: 1.8835 - binary_accuracy: 0.728 - ETA: 3:44 - loss: 1.8844 - binary_accuracy: 0.728 - ETA: 3:37 - loss: 1.8855 - binary_accuracy: 0.728 - ETA: 3:30 - loss: 1.8870 - binary_accuracy: 0.728 - ETA: 3:23 - loss: 1.8884 - binary_accuracy: 0.728 - ETA: 3:17 - loss: 1.8896 - binary_accuracy: 0.728 - ETA: 3:10 - loss: 1.8907 - binary_accuracy: 0.728 - ETA: 3:03 - loss: 1.8915 - binary_accuracy: 0.728 - ETA: 2:57 - loss: 1.8921 - binary_accuracy: 0.728 - ETA: 2:50 - loss: 1.8923 - binary_accuracy: 0.728 - ETA: 2:43 - loss: 1.8922 - binary_accuracy: 0.728 - ETA: 2:36 - loss: 1.8920 - binary_accuracy: 0.728 - ETA: 2:29 - loss: 1.8916 - binary_accuracy: 0.728 - ETA: 2:23 - loss: 1.8912 - binary_accuracy: 0.728 - ETA: 2:16 - loss: 1.8907 - binary_accuracy: 0.728 - ETA: 2:09 - loss: 1.8907 - binary_accuracy: 0.728 - ETA: 2:02 - loss: 1.8909 - binary_accuracy: 0.728 - ETA: 1:56 - loss: 1.8910 - binary_accuracy: 0.728 - ETA: 1:49 - loss: 1.8909 - binary_accuracy: 0.728 - ETA: 1:42 - loss: 1.8909 - binary_accuracy: 0.728 - ETA: 1:35 - loss: 1.8908 - binary_accuracy: 0.728 - ETA: 1:28 - loss: 1.8911 - binary_accuracy: 0.728 - ETA: 1:21 - loss: 1.8913 - binary_accuracy: 0.728 - ETA: 1:14 - loss: 1.8918 - binary_accuracy: 0.728 - ETA: 1:07 - loss: 1.8923 - binary_accuracy: 0.728 - ETA: 1:01 - loss: 1.8925 - binary_accuracy: 0.728 - ETA: 54s - loss: 1.8927 - binary_accuracy: 0.728 - ETA: 47s - loss: 1.8928 - binary_accuracy: 0.72 - ETA: 40s - loss: 1.8932 - binary_accuracy: 0.72 - ETA: 34s - loss: 1.8938 - binary_accuracy: 0.72 - ETA: 27s - loss: 1.8941 - binary_accuracy: 0.72 - ETA: 20s - loss: 1.8944 - binary_accuracy: 0.72 - ETA: 13s - loss: 1.8946 - binary_accuracy: 0.72 - ETA: 6s - loss: 1.8946 - binary_accuracy: 0.7278 - ETA: 0s - loss: 1.8947 - binary_accuracy: 0.727 - 487s 7s/step - loss: 1.8948 - binary_accuracy: 0.7277 - val_loss: 2.1879 - val_binary_accuracy: 0.7026\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])\n",
    "history = model.fit(x = valid_inputs, y = valid_outputs, epochs=10, validation_split = .1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 1065). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 1065). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 1layertransferlearn_10epoch_preprocessed\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 1layertransferlearn_10epoch_preprocessed\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"1layertransferlearn_10epoch_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 1065). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 1065). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 4layertransferlearn_20epoch_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 4layertransferlearn_20epoch_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"4layertransferlearn_20epoch_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 1065). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 1065). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 3layertransferlearn_10epoch_model_fullpreprocess\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 3layertransferlearn_10epoch_model_fullpreprocess\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"3layertransferlearn_10epoch_model_fullpreprocess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 1065). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 1065). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 3layertransferlearn_10epoch_model_fullpreprocess_v2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 3layertransferlearn_10epoch_model_fullpreprocess_v2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"3layertransferlearn_10epoch_model_fullpreprocess_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"3layertransferlearn_10epoch_model_fullpreprocess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['binary_accuracy'])\n",
    "plt.plot(history.history['val_binary_accuracy'])\n",
    "plt.title('Transfer Learning model loss')\n",
    "plt.ylabel('Binary Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99998647]\n",
      " [0.9999872 ]\n",
      " [0.99998295]\n",
      " ...\n",
      " [0.16755015]\n",
      " [0.99998045]\n",
      " [0.9999796 ]]\n",
      "        id                                               text\n",
      "2215  2216  If you are asian in the kitchen and african in...\n",
      "2582  2583  Why is there only a stairway to heaven but a h...\n",
      "1662  1663  I once dated a girl with a twin people asked m...\n",
      "3027  3028  There are people out their happier with less t...\n",
      "4343  4344  One zebra says to the other, \"i am going to ch...\n",
      "2215    1\n",
      "2582    1\n",
      "1662    1\n",
      "3027    0\n",
      "4343    1\n",
      "       ..\n",
      "1079    0\n",
      "7979    1\n",
      "1115    0\n",
      "6093    1\n",
      "6832    1\n",
      "Name: is_humor, Length: 1600, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(preds)\n",
    "print(X_test.head())\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = compute_output_arrays(y_test.to_frame(), ['is_humor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2314, 1) (579, 1)\n",
      "================== \n",
      "mean_absolute_error  : 0.2673404402310565\n",
      "mean_squared_error  : 0.25609044898018485\n",
      "r2 score  : -0.19905053361125913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25609044898018485"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(valid_outputs.shape, preds.shape)\n",
    "print_evaluation_metrics(np.array(outputs), np.array(preds), '', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lwing\\Downloads\\College\\Spring 2020\\M 340L\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== SPLIT on 0.1\n",
      "f1_score  : 0.8289611752360966\n",
      "[[ 21 158]\n",
      " [  5 395]]\n",
      "Acc 0.7184801381692574 Prec 0.7142857142857143 Rec 0.9875 F1 0.8289611752360966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lwing\\Downloads\\College\\Spring 2020\\M 340L\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== SPLIT on 0.2\n",
      "f1_score  : 0.8329809725158562\n",
      "[[ 27 152]\n",
      " [  6 394]]\n",
      "Acc 0.7271157167530224 Prec 0.7216117216117216 Rec 0.985 F1 0.8329809725158562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lwing\\Downloads\\College\\Spring 2020\\M 340L\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== SPLIT on 0.30000000000000004\n",
      "f1_score  : 0.8356309650053022\n",
      "[[ 30 149]\n",
      " [  6 394]]\n",
      "Acc 0.7322970639032815 Prec 0.7255985267034991 Rec 0.985 F1 0.8356309650053022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lwing\\Downloads\\College\\Spring 2020\\M 340L\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== SPLIT on 0.4\n",
      "f1_score  : 0.8343949044585988\n",
      "[[ 30 149]\n",
      " [  7 393]]\n",
      "Acc 0.7305699481865285 Prec 0.7250922509225092 Rec 0.9825 F1 0.8343949044585988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lwing\\Downloads\\College\\Spring 2020\\M 340L\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== SPLIT on 0.5\n",
      "f1_score  : 0.8361702127659575\n",
      "[[ 32 147]\n",
      " [  7 393]]\n",
      "Acc 0.7340241796200345 Prec 0.7277777777777777 Rec 0.9825 F1 0.8361702127659575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lwing\\Downloads\\College\\Spring 2020\\M 340L\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== SPLIT on 0.6\n",
      "f1_score  : 0.837953091684435\n",
      "[[ 34 145]\n",
      " [  7 393]]\n",
      "Acc 0.7374784110535406 Prec 0.7304832713754646 Rec 0.9825 F1 0.837953091684435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lwing\\Downloads\\College\\Spring 2020\\M 340L\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== SPLIT on 0.7000000000000001\n",
      "f1_score  : 0.8385026737967914\n",
      "[[ 36 143]\n",
      " [  8 392]]\n",
      "Acc 0.7392055267702936 Prec 0.7327102803738318 Rec 0.98 F1 0.8385026737967914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lwing\\Downloads\\College\\Spring 2020\\M 340L\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== SPLIT on 0.8\n",
      "f1_score  : 0.8344086021505376\n",
      "[[ 37 142]\n",
      " [ 12 388]]\n",
      "Acc 0.7340241796200345 Prec 0.7320754716981132 Rec 0.97 F1 0.8344086021505376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lwing\\Downloads\\College\\Spring 2020\\M 340L\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== SPLIT on 0.9\n",
      "f1_score  : 0.8382193268186753\n",
      "[[ 44 135]\n",
      " [ 14 386]]\n",
      "Acc 0.7426597582037997 Prec 0.7408829174664108 Rec 0.965 F1 0.8382193268186753\n"
     ]
    }
   ],
   "source": [
    "for split in np.arange(0.1, 0.99, 0.1).tolist():\n",
    "    X_test['pred_bi'] = (preds > split)\n",
    "\n",
    "    print_evaluation_metrics(y_test.to_frame()['is_humor'], X_test['pred_bi'], '', False, 'SPLIT on '+str(split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unrelated to paper; just trying out BERT tokenizer on william's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from gensim import utils\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.corpus import stopwords\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = 'bert-base-uncased'\n",
    "token = BertTokenizer.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_humor\n",
       "0         1\n",
       "1         1\n",
       "2         1\n",
       "3         1\n",
       "4         1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_is_humor.to_frame().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram Accuracy: 0.8580\n",
      "1-gram F1-Score: 0.8885\n",
      "\n",
      "2-gram Accuracy: 0.8605\n",
      "2-gram F1-Score: 0.8884\n",
      "\n",
      "3-gram Accuracy: 0.7795\n",
      "3-gram F1-Score: 0.8002\n",
      "\n",
      "4-gram Accuracy: 0.6085\n",
      "4-gram F1-Score: 0.5609\n",
      "\n",
      "5-gram Accuracy: 0.4980\n",
      "5-gram F1-Score: 0.3438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(1, 6):\n",
    "  # Pre-Processing\n",
    "  cv = CountVectorizer(ngram_range=(n,n), tokenizer=token.tokenize)\n",
    "  text_counts = cv.fit_transform(train_data['text'])\n",
    "  X_train, X_test, y_train, y_test = train_test_split(text_counts, train_is_humor.to_frame()['is_humor'], test_size=0.25, random_state=42)\n",
    "  \n",
    "  # Modeling\n",
    "  MNB = MultinomialNB()\n",
    "  MNB.fit(X_train, y_train)\n",
    "  predicted = MNB.predict(X_test)\n",
    "  accuracy_score = metrics.accuracy_score(y_test, predicted)\n",
    "  f1_score = metrics.f1_score(y_test, predicted)\n",
    "  print(f'{n}-gram Accuracy: {accuracy_score:.4f}')\n",
    "  print(f'{n}-gram F1-Score: {f1_score:.4f}')\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
